{"cells":[{"cell_type":"markdown","source":["# Use Delta Tables in Apache Spark - R version\n","R code by Antti Rask\n","\n","Original Python version: [https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03-delta-lake.html](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03-delta-lake.html)\n","\n","**Note!** This notebook was made primarily to be run inside Microsoft Fabric / Azure Synapse Data Engineering. You might have to make changes if you decide to open it in VS Code, for instance.\n","\n","Tables in a Microsoft Fabric lakehouse are based on the open source Delta Lake format for Apache Spark. Delta Lake adds support for relational semantics for both batch and streaming data operations, and enables the creation of a Lakehouse architecture in which Apache Spark can be used to process and query data in tables that are based on underlying files in a data lake.\n","\n","This exercise should take approximately **40** minutes to complete\n","\n","**Note:** You need a Microsoft Fabric trial to complete this exercise."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"15035f51-d2f9-4d15-8b43-b807e25af193"},{"cell_type":"markdown","source":["## Create a workspace\n","Before working with data in Fabric, create a workspace with the Fabric trial enabled.\n","\n","1. On the [Microsoft Fabric home page](https://app.fabric.microsoft.com/) at ``` https://app.fabric.microsoft.com ```, select **Synapse Data Engineering**.\n","2. In the menu bar on the left, select **Workspaces** (the icon looks similar to 🗇).\n","3. Create a new workspace with a name of your choice, selecting a licensing mode that includes Fabric capacity (Trial, Premium, or Fabric).\n","4. When your new workspace opens, it should be empty."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"75cd72d5-7678-4cc4-a93b-cc50fd2fc9d0"},{"cell_type":"markdown","source":["## Create a lakehouse and upload data\n","Now that you have a workspace, it’s time to create a data lakehouse for the data you’re going to analyze.\n","\n","1. In the **Synapse Data Engineering** home page, create a new **Lakehouse** with a name of your choice.\n","\n","After a minute or so, a new empty lakehouse. You need to ingest some data into the data lakehouse for analysis. There are multiple ways to do this, but in this exercise you’ll simply download a text file to your local computer (or lab VM if applicable) and then upload it to your lakehouse.\n","\n","2. Download the [data file](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) for this exercise from  ``` https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv ```, saving it as **products.csv** on your local computer (or lab VM if applicable).\n","\n","3. Return to the web browser tab containing your lakehouse, and in the **…** menu for the **Files** folder in the **Explorer** pane, select **New subfolder** and create a folder named **products**.\n","\n","4. In the **…** menu for the **products** folder, select **Upload** and **Upload files**, and then upload the **products.csv** file from your local computer (or lab VM if applicable) to the lakehouse.\n","\n","5. After the file has been uploaded, select the **products** folder; and verify that the **products.csv** file has been uploaded."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e6d5ec30-4dfb-451b-aed8-338dc954b564"},{"cell_type":"markdown","source":["## Housekeeping\n","\n","We'll get to the actual code soon, but first some housekeeping: installing and loading packages. And sorting out possible function conflicts.\n","\n","**Note:** Spark supports multiple coding languages, including Scala, Java, and R. In this exercise, we’ll use RSpark, which is a Spark-optimized variant of R. Although the heavy lifting will be done by [sparklyr](https://spark.posit.co/), a tidyverse friendly R interface to Apache Spark."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5c93628a-c8a6-4648-94dc-43b6431fd010"},{"cell_type":"markdown","source":["### Installing and loading needed R packages"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ba5189b1-7680-4dc3-8b0e-ef531bd9aee8"},{"cell_type":"code","source":["# Install packages\n","install.packages(\n","  c(\n","    \"conflicted\",\n","    \"dplyr\",\n","    \"sparklyr\"\n","  )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9129664Z","session_start_time":"2024-03-30T21:42:50.152765Z","execution_start_time":"2024-03-30T21:43:30.4571764Z","execution_finish_time":"2024-03-30T21:44:04.4321772Z","parent_msg_id":"c33d77f7-bf9b-4f29-b9d9-2b47cf3733ae"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 3, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Installing packages into ‘/nfs4/R/user-lib/application_1711834346503_0001’\n(as ‘lib’ is unspecified)\nalso installing the dependencies ‘tidyselect’, ‘dbplyr’\n\ntrying URL 'https://cloud.r-project.org/src/contrib/tidyselect_1.2.1.tar.gz'\nContent type 'application/x-gzip' length 103591 bytes (101 KB)\n==================================================\ndownloaded 101 KB\n\ntrying URL 'https://cloud.r-project.org/src/contrib/dbplyr_2.5.0.tar.gz'\nContent type 'application/x-gzip' length 770647 bytes (752 KB)\n==================================================\ndownloaded 752 KB\n\ntrying URL 'https://cloud.r-project.org/src/contrib/conflicted_1.2.0.tar.gz'\nContent type 'application/x-gzip' length 17071 bytes (16 KB)\n==================================================\ndownloaded 16 KB\n\ntrying URL 'https://cloud.r-project.org/src/contrib/dplyr_1.1.4.tar.gz'\nContent type 'application/x-gzip' length 1207521 bytes (1.2 MB)\n==================================================\ndownloaded 1.2 MB\n\ntrying URL 'https://cloud.r-project.org/src/contrib/sparklyr_1.8.5.tar.gz'\nContent type 'application/x-gzip' length 3210411 bytes (3.1 MB)\n==================================================\ndownloaded 3.1 MB\n\nLoading required package: usethis\n* installing *source* package ‘tidyselect’ ...\n** package ‘tidyselect’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\nLoading required package: usethis\nError in unloadNamespace(\"tidyselect\") : \n  namespace ‘tidyselect’ is imported by ‘tidyr’, ‘sparklyr’, ‘dplyr’, ‘dbplyr’ so cannot be unloaded\nExecution halted\nERROR: lazy loading failed for package ‘tidyselect’\n* removing ‘/nfs4/R/user-lib/application_1711834346503_0001/tidyselect’\nLoading required package: usethis\n* installing *source* package ‘conflicted’ ...\n** package ‘conflicted’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\nLoading required package: usethis\n** help\n*** installing help indices\n** building package indices\nLoading required package: usethis\n** testing if installed package can be loaded from temporary location\nLoading required package: usethis\n** testing if installed package can be loaded from final location\nLoading required package: usethis\n** testing if installed package keeps a record of temporary installation path\n* DONE (conflicted)\nLoading required package: usethis\n* installing *source* package ‘dplyr’ ...\n** package ‘dplyr’ successfully unpacked and MD5 sums checked\n** using staged installation\n** libs\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c chop.cpp -o chop.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c filter.cpp -o filter.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c funs.cpp -o funs.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c group_by.cpp -o group_by.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c group_data.cpp -o group_data.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c imports.cpp -o imports.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c init.cpp -o init.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c mask.cpp -o mask.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c mutate.cpp -o mutate.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c reconstruct.cpp -o reconstruct.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c slice.cpp -o slice.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -I/home/trusted-service-user/cluster-env/synapse_trident_r/include -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/trusted-service-user/cluster-env/synapse_trident_r/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1705202199365/work=/usr/local/src/conda/r-base-4.2.3 -fdebug-prefix-map=/home/trusted-service-user/cluster-env/synapse_trident_r=/usr/local/src/conda-prefix  -c summarise.cpp -o summarise.o\nx86_64-conda-linux-gnu-c++ -std=gnu++14 -shared -L/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/trusted-service-user/cluster-env/synapse_trident_r/lib -Wl,-rpath-link,/home/trusted-service-user/cluster-env/synapse_trident_r/lib -L/home/trusted-service-user/cluster-env/synapse_trident_r/lib -o dplyr.so chop.o filter.o funs.o group_by.o group_data.o imports.o init.o mask.o mutate.o reconstruct.o slice.o summarise.o -L/home/trusted-service-user/cluster-env/synapse_trident_r/lib/R/lib -lR\ninstalling to /nfs4/R/user-lib/application_1711834346503_0001/00LOCK-dplyr/00new/dplyr/libs\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\nLoading required package: usethis\nError in unloadNamespace(\"dplyr\") : \n  namespace ‘dplyr’ is imported by ‘tidyr’, ‘sparklyr’, ‘dbplyr’ so cannot be unloaded\nExecution halted\nERROR: lazy loading failed for package ‘dplyr’\n* removing ‘/nfs4/R/user-lib/application_1711834346503_0001/dplyr’\nLoading required package: usethis\n* installing *source* package ‘dbplyr’ ...\n** package ‘dbplyr’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\nError: package or namespace load failed for ‘sparklyr’ in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):\n namespace ‘tidyselect’ 1.2.0 is already loaded, but >= 1.2.1 is required\nExecution halted\nERROR: lazy loading failed for package ‘dbplyr’\n* removing ‘/nfs4/R/user-lib/application_1711834346503_0001/dbplyr’\nLoading required package: usethis\n* installing *source* package ‘sparklyr’ ...\n** package ‘sparklyr’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\nError: package or namespace load failed for ‘sparklyr’ in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):\n namespace ‘dbplyr’ 2.3.4 is being loaded, but >= 2.5.0 is required\nExecution halted\nERROR: lazy loading failed for package ‘sparklyr’\n* removing ‘/nfs4/R/user-lib/application_1711834346503_0001/sparklyr’\n\nThe downloaded source packages are in\n\t‘/tmp/Rtmpo8SBEu/downloaded_packages’\nWarning messages:\n1: In func(...) :\n  installation of package ‘tidyselect’ had non-zero exit status\n2: In func(...) : installation of package ‘dplyr’ had non-zero exit status\n3: In func(...) :\n  installation of package ‘dbplyr’ had non-zero exit status\n4: In func(...) :\n  installation of package ‘sparklyr’ had non-zero exit status"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5064e788-c3e2-4c53-b9dc-ea4f1adee354"},{"cell_type":"code","source":["# Load packages\n","library(conflicted) # Handling function conflicts between packages\n","library(dplyr)      # Data wrangling\n","library(sparklyr)   # Using Spark with R"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9196367Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:04.7933364Z","execution_finish_time":"2024-03-30T21:44:05.5525336Z","parent_msg_id":"04ee48b3-48b5-4f81-9791-a9df9e098053"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"beefdfd2-bbd1-47a9-bed0-41589d543e86"},{"cell_type":"markdown","source":["### Handling function conflicts between packages using the [{conflicted}](https://conflicted.r-lib.org/) package"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"227c110a-2a79-47a8-b0a4-ddd356b6a4b3"},{"cell_type":"code","source":["# Handling conflicts\n","conflicts_prefer(dplyr::summarize)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9202623Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:05.9389689Z","execution_finish_time":"2024-03-30T21:44:06.1576759Z","parent_msg_id":"8c216f80-4ce7-45e8-ab18-8d64f56c4ff7"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[conflicted] Will prefer dplyr::summarize over any other package."]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9706e1c4-7578-4ed1-95a3-fed25ea8aa17"},{"cell_type":"markdown","source":["### Setting up a Spark connection with {sparklyr}"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c0cfadd5-7d3c-419e-839e-94e9eeb8bac1"},{"cell_type":"code","source":["# Setting up sparklyr\n","spark_version <- sparkR.version()\n","config        <- spark_config()\n","sc            <- spark_connect(\n","    master  = \"yarn\",\n","    method  = \"synapse\",\n","    version = spark_version,\n","    config  = config\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.92084Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:06.4990805Z","execution_finish_time":"2024-03-30T21:44:07.9100614Z","parent_msg_id":"3ccbea73-f102-41e0-95b6-2734e694b2c1"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3770754f-9e5b-443a-9310-ded1f315b365"},{"cell_type":"markdown","source":["## Explore data in a dataframe\n","1. On the **Home** page while viewing the contents of the **products** folder in your datalake, in the **Open notebook** menu, select **New notebook**.\n","\n","After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).\n","\n","2. Select the existing cell in the notebook, which contains some simple code, and then use its 🗑 (Delete) icon at its top-right to remove it - you will not need this code.\n","\n","3. In the **Lakehouse explorer** pane on the left, expand **Files** and select **products** to reveal a new pane showing the **products.csv** file you uploaded previously.\n","\n","4. In the **…** menu for **products.csv**, select **Load data** > **Spark**. A new code cell containing the following code should be added to the notebook:\n","\n","**Tip:** You can hide the pane containing the files on the left by using its « icon. Doing so will help you focus on the notebook.\n","\n","5. Use the **▷ (Run cell)** button on the left of the cell to run it.\n","\n","**Note:** Since this is the first time you’ve run any Spark code in this notebook, a Spark session must be started. This means that the first run can take a minute or so to complete. Subsequent runs will be quicker.\n","\n","6. When the cell command has completed, review the output below the cell."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"694c47f1-3a75-46d0-a90d-57f19610a4e1"},{"cell_type":"code","source":["df <- spark_read_csv(\n","    sc,\n","    path = \"Files/products/products.csv\"\n",")\n","# df now is a Spark DataFrame containing CSV data from \"Files/products/products.csv\".\n","df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9213228Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:08.2610963Z","execution_finish_time":"2024-03-30T21:44:24.5746525Z","parent_msg_id":"d80cb58d-97f6-4be4-974e-2e305af57fe1"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 7, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["# Source: spark<products_7e1abce8_5676_4ef9_b259_c6b1ffeb2ff2> [?? x 4]\n   ProductID ProductName             Category       ListPrice\n       <int> <chr>                   <chr>              <dbl>\n 1       771 Mountain-100 Silver, 38 Mountain Bikes     3400.\n 2       772 Mountain-100 Silver, 42 Mountain Bikes     3400.\n 3       773 Mountain-100 Silver, 44 Mountain Bikes     3400.\n 4       774 Mountain-100 Silver, 48 Mountain Bikes     3400.\n 5       775 Mountain-100 Black, 38  Mountain Bikes     3375.\n 6       776 Mountain-100 Black, 42  Mountain Bikes     3375.\n 7       777 Mountain-100 Black, 44  Mountain Bikes     3375.\n 8       778 Mountain-100 Black, 48  Mountain Bikes     3375.\n 9       779 Mountain-200 Silver, 38 Mountain Bikes     2320.\n10       780 Mountain-200 Silver, 42 Mountain Bikes     2320.\n# ℹ more rows"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"808a2d03-5150-46a1-94b0-461473ca87da"},{"cell_type":"markdown","source":["## Create delta tables\n","You can save the dataframe as a delta table by using the **spark_write_table()** function and adding delta as a format in the options. Delta Lake supports the creation of both managed and external tables."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c2d96e35-e698-46b3-b630-f283cfde9e30"},{"cell_type":"markdown","source":["### Create a managed table\n","Managed tables are tables for which both the schema metadata and the data files are managed by Fabric. The data files for the table are created in the **Tables** folder.\n","\n","1. Under the results returned by the first code cell, use the **+ Code** icon to add a new code cell if one doesn’t already exist.\n","\n","**Tip:** To see the **+ Code** icon, move the mouse to just below and to the left of the output from the current cell. Alternatively, in the menu bar, on the **Edit** tab, select **+ Add code cell**.\n","\n","2. Enter the following code in the new cell and run it:\n","\n","3. In the **Lakehouse explorer** pane, in the **…** menu for the **Tables** folder, select **Refresh**. Then expand the **Tables** node and verify that the **managed_products** table has been created."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e849506a-0ec6-4d5c-9228-75d5fb221301"},{"cell_type":"code","source":["df %>%\n","    spark_write_table(\n","        name    = \"managed_products\",\n","        mode    = \"overwrite\",\n","        options = list(format = \"delta\")\n","    )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9267301Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:24.9147561Z","execution_finish_time":"2024-03-30T21:44:31.1630656Z","parent_msg_id":"98989c1d-fe26-45b4-88c3-f61e8439d0d4"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"97893f3f-d8d7-4c92-a5d6-f0274ddb8dc4"},{"cell_type":"markdown","source":["### Create an external table\n","You can also create external tables for which the schema metadata is defined in the metastore for the lakehouse, but the data files are stored in an external location.\n","\n","1. Add another new code cell, and add the following code to it:\n","\n","2. In the **Lakehouse explorer** pane, in the **…** menu for the **Files** folder, select **Copy ABFS path**.\n","\n","The ABFS path is the fully qualified path to the **Files** folder in the OneLake storage for your lakehouse - similar to this:\n","\n","_abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files_\n","\n","3. In the code you entered into the code cell, replace **abfs_path** with the path you copied to the clipboard so that the code saves the dataframe as an external table with data files in a folder named **external_products** in your **Files** folder location. The full path should look similar to this:\n","\n","_abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products_\n","\n","4. In the **Lakehouse explorer** pane, in the **…** menu for the **Tables** folder, select **Refresh**. Then expand the **Tables** node and verify that the **external_products** table has been created.\n","\n","5. In the **Lakehouse explorer** pane, in the **…** menu for the **Files** folder, select **Refresh**. Then expand the **Files** node and verify that the **external_products** folder has been created for the table’s data files."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7e7ac53e-be8b-4b93-a007-98e2aa2c1e7d"},{"cell_type":"code","source":["df %>%\n","    spark_write_table(\n","        name    = \"external_products\",\n","        mode    = \"overwrite\",\n","        options = list(\n","            format = \"delta\",\n","            path   = \"abfs_path/external_products\",\n","            type   = \"EXTERNAL\"\n","        )\n","    )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9273377Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:31.5014291Z","execution_finish_time":"2024-03-30T21:44:37.7713722Z","parent_msg_id":"45c0ea93-aeb7-46ee-9ce4-82034850ab0a"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 9, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d3673744-ec25-413f-ae05-e269948416d3"},{"cell_type":"markdown","source":["### Compare managed and external tables\n","Let’s explore the differences between managed and external tables.\n","\n","1. Add another code cell and run the following code:\n","\n","In the results, view the **Location** property for the table, which should be a path to the OneLake storage for the lakehouse ending with **/Tables/managed_products** (you may need to widen the **Data type** column to see the full path)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"af2a10df-d459-4afc-bd56-2d59c5b060cf"},{"cell_type":"code","source":["%%sql\n","\n","DESCRIBE FORMATTED managed_products;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9279877Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:38.1228959Z","execution_finish_time":"2024-03-30T21:44:38.86108Z","parent_msg_id":"3ce752d1-f9da-4419-99af-693a2a2837e0"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 10, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"col_name","type":"string","nullable":false,"metadata":{"comment":"name of the column"}},{"name":"data_type","type":"string","nullable":false,"metadata":{"comment":"data type of the column"}},{"name":"comment","type":"string","nullable":true,"metadata":{"comment":"comment of the column"}}]},"data":[["ProductID","int",null],["ProductName","string",null],["Category","string",null],["ListPrice","double",null],["","",""],["# Detailed Table Information","",""],["Name","spark_catalog.dp_uusiuusi.managed_products",""],["Type","MANAGED",""],["Location","abfss://3cb84d34-a5d7-41ef-91ee-d568529315ad@onelake.dfs.fabric.microsoft.com/bebf1738-538c-413d-8b17-0ffa6d0adc24/Tables/managed_products",""],["Provider","delta",""],["Owner","trusted-service-user",""],["Table Properties","[delta.minReaderVersion=1,delta.minWriterVersion=2]",""]]},"text/plain":"<Spark SQL result set with 12 rows and 3 fields>"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"315a19aa-7264-4729-89a0-24a1b7035fe5"},{"cell_type":"markdown","source":["2. Modify the ``` DESCRIBE ``` command to show the details of the external_products table as shown here:\n","\n","In the results, view the **Location** property for the table, which should be a path to the OneLake storage for the lakehouse ending with **/Files/external_products** (you may need to widen the **Data Type** column to see the full path).\n","\n","The files for managed table are stored in the **Tables** folder in the OneLake storage for the lakehouse. In this case, a folder named **managed_products** has been created to store the Parquet files and **delta_log** folder for the table you created."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"79e522a8-7a52-4419-9f1f-372a8ae70238"},{"cell_type":"code","source":["%%sql\n","\n","DESCRIBE FORMATTED external_products;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9286337Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:39.2180809Z","execution_finish_time":"2024-03-30T21:44:39.9481938Z","parent_msg_id":"a3ec450b-9ab5-40e7-98c5-8dbde147dd5f"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 11, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"col_name","type":"string","nullable":false,"metadata":{"comment":"name of the column"}},{"name":"data_type","type":"string","nullable":false,"metadata":{"comment":"data type of the column"}},{"name":"comment","type":"string","nullable":true,"metadata":{"comment":"comment of the column"}}]},"data":[["ProductID","int",null],["ProductName","string",null],["Category","string",null],["ListPrice","double",null],["","",""],["# Detailed Table Information","",""],["Name","spark_catalog.dp_uusiuusi.external_products",""],["Type","EXTERNAL",""],["Location","abfss://dp_uusi@onelake.dfs.fabric.microsoft.com/dp_uusiuusi.Lakehouse/Files/external_products",""],["Provider","delta",""],["Owner","trusted-service-user",""],["Table Properties","[delta.minReaderVersion=1,delta.minWriterVersion=2]",""]]},"text/plain":"<Spark SQL result set with 12 rows and 3 fields>"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"b3d09d92-9946-4969-a8ba-d7371332e10b"},{"cell_type":"markdown","source":["3. Add another code cell and run the following code:\n","\n","4. In the **Lakehouse explorer** pane, in the **…** menu for the **Tables** folder, select **Refresh**. Then expand the **Tables** node and verify that no tables are listed.\n","\n","5. In the **Lakehouse explorer** pane, expand the **Files** folder and verify that the **external_products** has not been deleted. Select this folder to view the Parquet data files and **_delta_log** folder for the data that was previously in the **external_products** table. The table metadata for the external table was deleted, but the files were not affected."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fd0b009d-e5eb-4284-814c-93c689422cec"},{"cell_type":"code","source":["%%sql\n","\n","DROP TABLE managed_products;\n","DROP TABLE external_products;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":-1,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:42:49.9293154Z","session_start_time":null,"execution_start_time":"2024-03-30T21:44:43.6891092Z","execution_finish_time":"2024-03-30T21:44:43.6892464Z","parent_msg_id":"eecb5e8a-9f9a-40d5-a533-1f8345440af0"},"text/plain":"StatementMeta(, , -1, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":10,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":10,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"a6b8380d-724a-4f27-bd0d-1b02c56942af"},{"cell_type":"markdown","source":["### Use SQL to create a table\n","1. Add another code cell and run the following code:\n","\n","**Note!** I added the DROP TABLE in case you want to rerun the whole notebook. Otherwise the cell will produce an error.\n","\n","2. In the **Lakehouse explorer** pane, in the **…** menu for the **Tables** folder, select **Refresh**. Then expand the **Tables** node and verify that a new table named **products** is listed. Then expand the table to verify that its schema matches the original dataframe that was saved in the **external_products** folder."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eba2bd3b-8e46-4fec-aa39-22fa0ea999ea"},{"cell_type":"code","source":["%%sql\n","\n","DROP TABLE products;\n","\n","CREATE TABLE products\n","USING DELTA\n","LOCATION 'Files/external_products';"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":-1,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:47:08.2710685Z","session_start_time":null,"execution_start_time":"2024-03-30T21:47:12.7457294Z","execution_finish_time":"2024-03-30T21:47:12.7458788Z","parent_msg_id":"74ac3e9f-8574-406d-9c68-e6e0c2e1289b"},"text/plain":"StatementMeta(, , -1, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"96bebbec-0568-4865-8526-4fcb2c7d8d50"},{"cell_type":"markdown","source":["3. Add another code cell and run the following code:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"10e79faa-d1b0-4fd2-89e4-bd2353cf3cda"},{"cell_type":"code","source":["%%sql\n","\n","SELECT * FROM products;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:45:37.141086Z","session_start_time":null,"execution_start_time":"2024-03-30T21:45:37.5281159Z","execution_finish_time":"2024-03-30T21:45:48.0583792Z","parent_msg_id":"e329e00d-357a-4250-9902-8aebdd081c43"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 16, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":13,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"ProductID","type":"integer","nullable":true,"metadata":{}},{"name":"ProductName","type":"string","nullable":true,"metadata":{}},{"name":"Category","type":"string","nullable":true,"metadata":{}},{"name":"ListPrice","type":"double","nullable":true,"metadata":{}}]},"data":[[771,"Mountain-100 Silver, 38","Mountain Bikes",3399.99],[772,"Mountain-100 Silver, 42","Mountain Bikes",3399.99],[773,"Mountain-100 Silver, 44","Mountain Bikes",3399.99],[774,"Mountain-100 Silver, 48","Mountain Bikes",3399.99],[775,"Mountain-100 Black, 38","Mountain Bikes",3374.99],[776,"Mountain-100 Black, 42","Mountain Bikes",3374.99],[777,"Mountain-100 Black, 44","Mountain Bikes",3374.99],[778,"Mountain-100 Black, 48","Mountain Bikes",3374.99],[779,"Mountain-200 Silver, 38","Mountain Bikes",2319.99],[780,"Mountain-200 Silver, 42","Mountain Bikes",2319.99],[781,"Mountain-200 Silver, 46","Mountain Bikes",2319.99],[782,"Mountain-200 Black, 38","Mountain Bikes",2294.99],[783,"Mountain-200 Black, 42","Mountain Bikes",2294.99],[784,"Mountain-200 Black, 46","Mountain Bikes",2294.99],[785,"Mountain-300 Black, 38","Mountain Bikes",1079.99],[786,"Mountain-300 Black, 40","Mountain Bikes",1079.99],[787,"Mountain-300 Black, 44","Mountain Bikes",1079.99],[788,"Mountain-300 Black, 48","Mountain Bikes",1079.99],[980,"Mountain-400-W Silver, 38","Mountain Bikes",769.49],[981,"Mountain-400-W Silver, 40","Mountain Bikes",769.49],[982,"Mountain-400-W Silver, 42","Mountain Bikes",769.49],[983,"Mountain-400-W Silver, 46","Mountain Bikes",769.49],[984,"Mountain-500 Silver, 40","Mountain Bikes",564.99],[985,"Mountain-500 Silver, 42","Mountain Bikes",564.99],[986,"Mountain-500 Silver, 44","Mountain Bikes",564.99],[987,"Mountain-500 Silver, 48","Mountain Bikes",564.99],[988,"Mountain-500 Silver, 52","Mountain Bikes",564.99],[989,"Mountain-500 Black, 40","Mountain Bikes",539.99],[990,"Mountain-500 Black, 42","Mountain Bikes",539.99],[991,"Mountain-500 Black, 44","Mountain Bikes",539.99],[992,"Mountain-500 Black, 48","Mountain Bikes",539.99],[993,"Mountain-500 Black, 52","Mountain Bikes",539.99],[978,"Touring-3000 Blue, 44","Touring Bikes",742.35],[979,"Touring-3000 Blue, 50","Touring Bikes",742.35],[953,"Touring-2000 Blue, 60","Touring Bikes",1214.85],[954,"Touring-1000 Yellow, 46","Touring Bikes",2384.07],[955,"Touring-1000 Yellow, 50","Touring Bikes",2384.07],[956,"Touring-1000 Yellow, 54","Touring Bikes",2384.07],[957,"Touring-1000 Yellow, 60","Touring Bikes",2384.07],[958,"Touring-3000 Blue, 54","Touring Bikes",742.35],[959,"Touring-3000 Blue, 58","Touring Bikes",742.35],[960,"Touring-3000 Blue, 62","Touring Bikes",742.35],[961,"Touring-3000 Yellow, 44","Touring Bikes",742.35],[962,"Touring-3000 Yellow, 50","Touring Bikes",742.35],[963,"Touring-3000 Yellow, 54","Touring Bikes",742.35],[964,"Touring-3000 Yellow, 58","Touring Bikes",742.35],[965,"Touring-3000 Yellow, 62","Touring Bikes",742.35],[966,"Touring-1000 Blue, 46","Touring Bikes",2384.07],[967,"Touring-1000 Blue, 50","Touring Bikes",2384.07],[968,"Touring-1000 Blue, 54","Touring Bikes",2384.07],[969,"Touring-1000 Blue, 60","Touring Bikes",2384.07],[970,"Touring-2000 Blue, 46","Touring Bikes",1214.85],[971,"Touring-2000 Blue, 50","Touring Bikes",1214.85],[972,"Touring-2000 Blue, 54","Touring Bikes",1214.85],[946,"LL Touring Handlebars","Handlebars",46.09],[947,"HL Touring Handlebars","Handlebars",91.57],[808,"LL Mountain Handlebars","Handlebars",44.54],[809,"ML Mountain Handlebars","Handlebars",61.92],[810,"HL Mountain Handlebars","Handlebars",120.27],[811,"LL Road Handlebars","Handlebars",44.54],[812,"ML Road Handlebars","Handlebars",61.92],[813,"HL Road Handlebars","Handlebars",120.27],[994,"LL Bottom Bracket","Bottom Brackets",53.99],[995,"ML Bottom Bracket","Bottom Brackets",101.24],[996,"HL Bottom Bracket","Bottom Brackets",121.49],[948,"Front Brakes","Brakes",106.5],[907,"Rear Brakes","Brakes",106.5],[952,"Chain","Chains",20.24],[949,"LL Crankset","Cranksets",175.49],[950,"ML Crankset","Cranksets",256.49],[951,"HL Crankset","Cranksets",404.99],[945,"Front Derailleur","Derailleurs",91.49],[894,"Rear Derailleur","Derailleurs",121.46],[802,"LL Fork","Forks",148.22],[803,"ML Fork","Forks",175.49],[804,"HL Fork","Forks",229.49],[805,"LL Headset","Headsets",34.2],[806,"ML Headset","Headsets",102.29],[807,"HL Headset","Headsets",124.73],[739,"HL Mountain Frame - Silver, 42","Mountain Frames",1364.5],[740,"HL Mountain Frame - Silver, 44","Mountain Frames",1364.5],[741,"HL Mountain Frame - Silver, 48","Mountain Frames",1364.5],[742,"HL Mountain Frame - Silver, 46","Mountain Frames",1364.5],[743,"HL Mountain Frame - Black, 42","Mountain Frames",1349.6],[744,"HL Mountain Frame - Black, 44","Mountain Frames",1349.6],[745,"HL Mountain Frame - Black, 48","Mountain Frames",1349.6],[746,"HL Mountain Frame - Black, 46","Mountain Frames",1349.6],[747,"HL Mountain Frame - Black, 38","Mountain Frames",1349.6],[748,"HL Mountain Frame - Silver, 38","Mountain Frames",1364.5],[814,"ML Mountain Frame - Black, 38","Mountain Frames",348.76],[830,"ML Mountain Frame - Black, 40","Mountain Frames",348.76],[831,"ML Mountain Frame - Black, 44","Mountain Frames",348.76],[832,"ML Mountain Frame - Black, 48","Mountain Frames",348.76],[924,"LL Mountain Frame - Black, 42","Mountain Frames",249.79],[925,"LL Mountain Frame - Black, 44","Mountain Frames",249.79],[926,"LL Mountain Frame - Black, 48","Mountain Frames",249.79],[927,"LL Mountain Frame - Black, 52","Mountain Frames",249.79],[917,"LL Mountain Frame - Silver, 42","Mountain Frames",264.05],[918,"LL Mountain Frame - Silver, 44","Mountain Frames",264.05],[919,"LL Mountain Frame - Silver, 48","Mountain Frames",264.05],[920,"LL Mountain Frame - Silver, 52","Mountain Frames",264.05],[904,"ML Mountain Frame-W - Silver, 40","Mountain Frames",364.09],[905,"ML Mountain Frame-W - Silver, 42","Mountain Frames",364.09],[906,"ML Mountain Frame-W - Silver, 46","Mountain Frames",364.09],[942,"ML Mountain Frame-W - Silver, 38","Mountain Frames",364.09],[943,"LL Mountain Frame - Black, 40","Mountain Frames",249.79],[944,"LL Mountain Frame - Silver, 40","Mountain Frames",264.05],[935,"LL Mountain Pedal","Pedals",40.49],[936,"ML Mountain Pedal","Pedals",62.09],[937,"HL Mountain Pedal","Pedals",80.99],[938,"LL Road Pedal","Pedals",40.49],[939,"ML Road Pedal","Pedals",62.09],[940,"HL Road Pedal","Pedals",80.99],[941,"Touring Pedal","Pedals",80.99],[908,"LL Mountain Seat/Saddle","Saddles",27.12],[909,"ML Mountain Seat/Saddle","Saddles",39.14],[910,"HL Mountain Seat/Saddle","Saddles",52.64],[911,"LL Road Seat/Saddle","Saddles",27.12],[912,"ML Road Seat/Saddle","Saddles",39.14],[913,"HL Road Seat/Saddle","Saddles",52.64],[914,"LL Touring Seat/Saddle","Saddles",27.12],[915,"ML Touring Seat/Saddle","Saddles",39.14],[916,"HL Touring Seat/Saddle","Saddles",52.64],[895,"LL Touring Frame - Blue, 50","Touring Frames",333.42],[896,"LL Touring Frame - Blue, 54","Touring Frames",333.42],[897,"LL Touring Frame - Blue, 58","Touring Frames",333.42],[898,"LL Touring Frame - Blue, 62","Touring Frames",333.42],[899,"LL Touring Frame - Yellow, 44","Touring Frames",333.42],[900,"LL Touring Frame - Yellow, 50","Touring Frames",333.42],[901,"LL Touring Frame - Yellow, 54","Touring Frames",333.42],[902,"LL Touring Frame - Yellow, 58","Touring Frames",333.42],[903,"LL Touring Frame - Blue, 44","Touring Frames",333.42],[885,"HL Touring Frame - Yellow, 60","Touring Frames",1003.91],[886,"LL Touring Frame - Yellow, 62","Touring Frames",333.42],[887,"HL Touring Frame - Yellow, 46","Touring Frames",1003.91],[888,"HL Touring Frame - Yellow, 50","Touring Frames",1003.91],[889,"HL Touring Frame - Yellow, 54","Touring Frames",1003.91],[890,"HL Touring Frame - Blue, 46","Touring Frames",1003.91],[891,"HL Touring Frame - Blue, 50","Touring Frames",1003.91],[892,"HL Touring Frame - Blue, 54","Touring Frames",1003.91],[893,"HL Touring Frame - Blue, 60","Touring Frames",1003.91],[823,"LL Mountain Rear Wheel","Wheels",87.745],[824,"ML Mountain Rear Wheel","Wheels",236.025],[825,"HL Mountain Rear Wheel","Wheels",327.215],[826,"LL Road Rear Wheel","Wheels",112.565],[827,"ML Road Rear Wheel","Wheels",275.385],[828,"HL Road Rear Wheel","Wheels",357.06],[829,"Touring Rear Wheel","Wheels",245.01],[815,"LL Mountain Front Wheel","Wheels",60.745],[816,"ML Mountain Front Wheel","Wheels",209.025],[817,"HL Mountain Front Wheel","Wheels",300.215],[818,"LL Road Front Wheel","Wheels",85.565],[819,"ML Road Front Wheel","Wheels",248.385],[820,"HL Road Front Wheel","Wheels",330.06],[821,"Touring Front Wheel","Wheels",218.01],[855,"Men's Bib-Shorts, S","Bib-Shorts",89.99],[856,"Men's Bib-Shorts, M","Bib-Shorts",89.99],[857,"Men's Bib-Shorts, L","Bib-Shorts",89.99],[712,"AWC Logo Cap","Caps",8.99],[858,"Half-Finger Gloves, S","Gloves",24.49],[859,"Half-Finger Gloves, M","Gloves",24.49],[860,"Half-Finger Gloves, L","Gloves",24.49],[861,"Full-Finger Gloves, S","Gloves",37.99],[862,"Full-Finger Gloves, M","Gloves",37.99],[863,"Full-Finger Gloves, L","Gloves",37.99],[713,"Long-Sleeve Logo Jersey, S","Jerseys",49.99],[714,"Long-Sleeve Logo Jersey, M","Jerseys",49.99],[715,"Long-Sleeve Logo Jersey, L","Jerseys",49.99],[716,"Long-Sleeve Logo Jersey, XL","Jerseys",49.99],[881,"Short-Sleeve Classic Jersey, S","Jerseys",53.99],[882,"Short-Sleeve Classic Jersey, M","Jerseys",53.99],[883,"Short-Sleeve Classic Jersey, L","Jerseys",53.99],[884,"Short-Sleeve Classic Jersey, XL","Jerseys",53.99],[867,"Women's Mountain Shorts, S","Shorts",69.99],[868,"Women's Mountain Shorts, M","Shorts",69.99],[869,"Women's Mountain Shorts, L","Shorts",69.99],[841,"Men's Sports Shorts, S","Shorts",59.99],[849,"Men's Sports Shorts, M","Shorts",59.99],[850,"Men's Sports Shorts, L","Shorts",59.99],[851,"Men's Sports Shorts, XL","Shorts",59.99],[709,"Mountain Bike Socks, M","Socks",9.5],[710,"Mountain Bike Socks, L","Socks",9.5],[874,"Racing Socks, M","Socks",8.99],[875,"Racing Socks, L","Socks",8.99],[852,"Women's Tights, S","Tights",74.99],[853,"Women's Tights, M","Tights",74.99],[854,"Women's Tights, L","Tights",74.99],[864,"Classic Vest, S","Vests",63.5],[865,"Classic Vest, M","Vests",63.5],[866,"Classic Vest, L","Vests",63.5],[876,"Hitch Rack - 4-Bike","Bike Racks",120],[879,"All-Purpose Bike Stand","Bike Stands",159],[870,"Water Bottle - 30 oz.","Bottles and Cages",4.99],[871,"Mountain Bottle Cage","Bottles and Cages",9.99],[872,"Road Bottle Cage","Bottles and Cages",8.99],[877,"Bike Wash - Dissolver","Cleaners",7.95],[878,"Fender Set - Mountain","Fenders",21.98],[711,"Sport-100 Helmet, Blue","Helmets",34.99],[707,"Sport-100 Helmet, Red","Helmets",34.99],[708,"Sport-100 Helmet, Black","Helmets",34.99],[880,"Hydration Pack - 70 oz.","Hydration Packs",54.99],[846,"Taillights - Battery-Powered","Lights",13.99],[847,"Headlights - Dual-Beam","Lights",34.99],[848,"Headlights - Weatherproof","Lights",44.99],[843,"Cable Lock","Locks",25],[842,"Touring-Panniers, Large","Panniers",125],[844,"Minipump","Pumps",19.99],[845,"Mountain Pump","Pumps",24.99],[873,"Patch Kit/8 Patches","Tires and Tubes",2.29],[921,"Mountain Tire Tube","Tires and Tubes",4.99],[922,"Road Tire Tube","Tires and Tubes",3.99],[923,"Touring Tire Tube","Tires and Tubes",4.99],[928,"LL Mountain Tire","Tires and Tubes",24.99],[929,"ML Mountain Tire","Tires and Tubes",29.99],[930,"HL Mountain Tire","Tires and Tubes",35],[931,"LL Road Tire","Tires and Tubes",21.49],[932,"ML Road Tire","Tires and Tubes",24.99],[933,"HL Road Tire","Tires and Tubes",32.6],[934,"Touring Tire","Tires and Tubes",28.99],[833,"ML Road Frame-W - Yellow, 40","Road Frames",594.83],[834,"ML Road Frame-W - Yellow, 42","Road Frames",594.83],[835,"ML Road Frame-W - Yellow, 44","Road Frames",594.83],[836,"ML Road Frame-W - Yellow, 48","Road Frames",594.83],[837,"HL Road Frame - Black, 62","Road Frames",1431.5],[838,"HL Road Frame - Black, 44","Road Frames",1431.5],[839,"HL Road Frame - Black, 48","Road Frames",1431.5],[840,"HL Road Frame - Black, 52","Road Frames",1431.5],[822,"ML Road Frame-W - Yellow, 38","Road Frames",594.83],[680,"HL Road Frame - Black, 58","Road Frames",1431.5],[706,"HL Road Frame - Red, 58","Road Frames",1431.5],[717,"HL Road Frame - Red, 62","Road Frames",1431.5],[718,"HL Road Frame - Red, 44","Road Frames",1431.5],[719,"HL Road Frame - Red, 48","Road Frames",1431.5],[720,"HL Road Frame - Red, 52","Road Frames",1431.5],[721,"HL Road Frame - Red, 56","Road Frames",1431.5],[722,"LL Road Frame - Black, 58","Road Frames",337.22],[723,"LL Road Frame - Black, 60","Road Frames",337.22],[724,"LL Road Frame - Black, 62","Road Frames",337.22],[725,"LL Road Frame - Red, 44","Road Frames",337.22],[726,"LL Road Frame - Red, 48","Road Frames",337.22],[727,"LL Road Frame - Red, 52","Road Frames",337.22],[728,"LL Road Frame - Red, 58","Road Frames",337.22],[729,"LL Road Frame - Red, 60","Road Frames",337.22],[730,"LL Road Frame - Red, 62","Road Frames",337.22],[731,"ML Road Frame - Red, 44","Road Frames",594.83],[732,"ML Road Frame - Red, 48","Road Frames",594.83],[733,"ML Road Frame - Red, 52","Road Frames",594.83],[734,"ML Road Frame - Red, 58","Road Frames",594.83],[735,"ML Road Frame - Red, 60","Road Frames",594.83],[736,"LL Road Frame - Black, 44","Road Frames",337.22],[737,"LL Road Frame - Black, 48","Road Frames",337.22],[738,"LL Road Frame - Black, 52","Road Frames",337.22],[973,"Road-350-W Yellow, 40","Road Bikes",1700.99],[974,"Road-350-W Yellow, 42","Road Bikes",1700.99],[975,"Road-350-W Yellow, 44","Road Bikes",1700.99],[976,"Road-350-W Yellow, 48","Road Bikes",1700.99],[977,"Road-750 Black, 58","Road Bikes",539.99],[997,"Road-750 Black, 44","Road Bikes",539.99],[998,"Road-750 Black, 48","Road Bikes",539.99],[999,"Road-750 Black, 52","Road Bikes",539.99],[797,"Road-550-W Yellow, 38","Road Bikes",1120.49],[798,"Road-550-W Yellow, 40","Road Bikes",1120.49],[799,"Road-550-W Yellow, 42","Road Bikes",1120.49],[800,"Road-550-W Yellow, 44","Road Bikes",1120.49],[801,"Road-550-W Yellow, 48","Road Bikes",1120.49],[749,"Road-150 Red, 62","Road Bikes",3578.27],[750,"Road-150 Red, 44","Road Bikes",3578.27],[751,"Road-150 Red, 48","Road Bikes",3578.27],[752,"Road-150 Red, 52","Road Bikes",3578.27],[753,"Road-150 Red, 56","Road Bikes",3578.27],[754,"Road-450 Red, 58","Road Bikes",1457.99],[755,"Road-450 Red, 60","Road Bikes",1457.99],[756,"Road-450 Red, 44","Road Bikes",1457.99],[757,"Road-450 Red, 48","Road Bikes",1457.99],[758,"Road-450 Red, 52","Road Bikes",1457.99],[789,"Road-250 Red, 44","Road Bikes",2443.35],[790,"Road-250 Red, 48","Road Bikes",2443.35],[791,"Road-250 Red, 52","Road Bikes",2443.35],[792,"Road-250 Red, 58","Road Bikes",2443.35],[793,"Road-250 Black, 44","Road Bikes",2443.35],[794,"Road-250 Black, 48","Road Bikes",2443.35],[795,"Road-250 Black, 52","Road Bikes",2443.35],[796,"Road-250 Black, 58","Road Bikes",2443.35],[759,"Road-650 Red, 58","Road Bikes",782.99],[760,"Road-650 Red, 60","Road Bikes",782.99],[761,"Road-650 Red, 62","Road Bikes",782.99],[762,"Road-650 Red, 44","Road Bikes",782.99],[763,"Road-650 Red, 48","Road Bikes",782.99],[764,"Road-650 Red, 52","Road Bikes",782.99],[765,"Road-650 Black, 58","Road Bikes",782.99],[766,"Road-650 Black, 60","Road Bikes",782.99],[767,"Road-650 Black, 62","Road Bikes",782.99],[768,"Road-650 Black, 44","Road Bikes",782.99],[769,"Road-650 Black, 48","Road Bikes",782.99],[770,"Road-650 Black, 52","Road Bikes",782.99]]},"text/plain":"<Spark SQL result set with 295 rows and 4 fields>"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"eabaafbf-50a0-4b2c-83be-352ec9376b73"},{"cell_type":"markdown","source":["## Explore table versioning\n","Transaction history for delta tables is stored in JSON files in the **delta_log** folder. You can use this transaction log to manage data versioning.\n","\n","1. Add a new code cell to the notebook and run the following code:\n","\n","This code implements a 10% reduction in the price for mountain bikes."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"193a540a-cd55-4611-9100-f1b349ce2678"},{"cell_type":"code","source":["%%sql\n","\n","UPDATE products\n","SET ListPrice = ListPrice * 0.9\n","WHERE Category = 'Mountain Bikes';"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":19,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:47:46.8640844Z","session_start_time":null,"execution_start_time":"2024-03-30T21:47:47.1998419Z","execution_finish_time":"2024-03-30T21:47:53.4624766Z","parent_msg_id":"8d1c2689-ce2c-4feb-96ef-88856577c063"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 19, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":15,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"num_affected_rows","type":"long","nullable":true,"metadata":{}}]},"data":[["32"]]},"text/plain":"<Spark SQL result set with 1 rows and 1 fields>"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"aba778ed-a709-4f65-9895-714273c3052b"},{"cell_type":"markdown","source":["2. Add another code cell and run the following code:\n","\n","The results show the history of transactions recorded for the table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e5900d23-63ee-4fc5-95ad-f433b526fcd6"},{"cell_type":"code","source":["%%sql\n","\n","DESCRIBE HISTORY products;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":20,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:47:56.0655303Z","session_start_time":null,"execution_start_time":"2024-03-30T21:47:56.3940812Z","execution_finish_time":"2024-03-30T21:47:58.7302365Z","parent_msg_id":"5cb3d7f2-454d-4e2d-998d-f6991203dbe3"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 20, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":16,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"version","type":"long","nullable":true,"metadata":{}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{}},{"name":"userId","type":"string","nullable":true,"metadata":{}},{"name":"userName","type":"string","nullable":true,"metadata":{}},{"name":"operation","type":"string","nullable":true,"metadata":{}},{"name":"operationParameters","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"job","type":{"type":"struct","fields":[{"name":"jobId","type":"string","nullable":true,"metadata":{}},{"name":"jobName","type":"string","nullable":true,"metadata":{}},{"name":"runId","type":"string","nullable":true,"metadata":{}},{"name":"jobOwnerId","type":"string","nullable":true,"metadata":{}},{"name":"triggerType","type":"string","nullable":true,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"notebook","type":{"type":"struct","fields":[{"name":"notebookId","type":"string","nullable":true,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"clusterId","type":"string","nullable":true,"metadata":{}},{"name":"readVersion","type":"long","nullable":true,"metadata":{}},{"name":"isolationLevel","type":"string","nullable":true,"metadata":{}},{"name":"isBlindAppend","type":"boolean","nullable":true,"metadata":{}},{"name":"operationMetrics","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"userMetadata","type":"string","nullable":true,"metadata":{}},{"name":"engineInfo","type":"string","nullable":true,"metadata":{}}]},"data":[["3","2024-03-30T21:47:51Z",null,null,"UPDATE",{"predicate":"[\"(Category#3021 = Mountain Bikes)\"]"},null,null,null,"2","Serializable",false,{"numRemovedFiles":"1","numRemovedBytes":"7602","numCopiedRows":"263","numAddedChangeFiles":"0","executionTimeMs":"3620","scanTimeMs":"1032","numAddedFiles":"1","numUpdatedRows":"32","numAddedBytes":"7659","rewriteTimeMs":"2586"},null,"Apache-Spark/3.4.1.5.3-117503204 Delta-Lake/2.4.0.9"],["2","2024-03-30T21:44:34Z",null,null,"CREATE OR REPLACE TABLE AS SELECT",{"isManaged":"false","description":null,"partitionBy":"[]","properties":"{}"},null,null,null,"1","Serializable",false,{"numFiles":"1","numOutputRows":"295","numOutputBytes":"7602"},null,"Apache-Spark/3.4.1.5.3-117503204 Delta-Lake/2.4.0.9"],["1","2024-03-30T20:01:21Z",null,null,"UPDATE",{"predicate":"[\"(Category#10226 = Mountain Bikes)\"]"},null,null,null,"0","Serializable",false,{"numRemovedFiles":"1","numRemovedBytes":"7602","numCopiedRows":"263","numAddedChangeFiles":"0","executionTimeMs":"1197","scanTimeMs":"394","numAddedFiles":"1","numUpdatedRows":"32","numAddedBytes":"7659","rewriteTimeMs":"803"},null,"Apache-Spark/3.4.1.5.3-117503204 Delta-Lake/2.4.0.9"],["0","2024-03-30T19:58:07Z",null,null,"CREATE OR REPLACE TABLE AS SELECT",{"isManaged":"false","description":null,"partitionBy":"[]","properties":"{}"},null,null,null,null,"Serializable",false,{"numFiles":"1","numOutputRows":"295","numOutputBytes":"7602"},null,"Apache-Spark/3.4.1.5.3-117503204 Delta-Lake/2.4.0.9"]]},"text/plain":"<Spark SQL result set with 4 rows and 15 fields>"},"metadata":{}}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"f30b7df6-4368-46f2-8299-e464702b8613"},{"cell_type":"markdown","source":["3. Add another code cell and run the following code:\n","\n","The results show two dataframes - one containing the data after the price reduction, and the other showing the original version of the data."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9b35b45c-e304-4386-8d78-21be2190c3b9"},{"cell_type":"code","source":["delta_table_path <- \"Files/external_products\"\n","\n","# Get the current data\n","current_data <- spark_read_delta(sc, path = delta_table_path)\n","\n","current_data %>% \n","    summarize(sum = sum(ListPrice, na.rm = TRUE))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":24,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:48:54.7412055Z","session_start_time":null,"execution_start_time":"2024-03-30T21:48:55.0664326Z","execution_finish_time":"2024-03-30T21:48:56.5929604Z","parent_msg_id":"4198a237-1d8f-43a3-bdb6-143823953a46"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 24, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["# Source: spark<?> [?? x 1]\n      sum\n    <dbl>\n1 214269."]}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"c6573a33-2973-4bff-a936-8b0b3f4f1e1e"},{"cell_type":"code","source":["# Get the version 0 data\n","original_data <- spark_read_delta(\n","    sc,\n","    path    = delta_table_path,\n","    version = 0\n",")\n","original_data %>% \n","    summarize(sum = sum(ListPrice))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":22,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:48:21.6613683Z","session_start_time":null,"execution_start_time":"2024-03-30T21:48:21.9803643Z","execution_finish_time":"2024-03-30T21:48:25.4471589Z","parent_msg_id":"af303afd-ea18-4ee5-917a-ffb670f0a6d3"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 22, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["# Source: spark<?> [?? x 1]\n      sum\n    <dbl>\n1 219656."]}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"cbbdfafc-f5fa-4cb1-bfd2-d8047b652373"},{"cell_type":"markdown","source":["## Use delta tables for streaming data\n","**Note!** SparkR isn't yet supported for streaming data. That's why this part is using PySpark code.\n","\n","Delta lake supports streaming data. Delta tables can be a sink or a source for data streams created using the Spark Structured Streaming API. In this example, you’ll use a delta table as a sink for some streaming data in a simulated internet of things (IoT) scenario.\n","\n","1. Add a new code cell in the notebook. Then, in the new cell, add the following code and run it:\n","\n","Ensure the message Source stream created… is printed. The code you just ran has created a streaming data source based on a folder to which some data has been saved, representing readings from hypothetical IoT devices."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c400a021-6a52-4a3b-8956-0b4a7db47098"},{"cell_type":"code","source":["%%pyspark\n","\n","from notebookutils         import mssparkutils\n","from pyspark.sql.types     import *\n","from pyspark.sql.functions import *\n","\n","# Create a folder\n","inputPath = 'Files/data/'\n","mssparkutils.fs.mkdirs(inputPath)\n","\n","# Create a stream that reads data from the folder, using a JSON schema\n","jsonSchema = StructType([\n","StructField(\"device\", StringType(), False),\n","StructField(\"status\", StringType(), False)\n","])\n","iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n","\n","# Write some event data to the folder\n","device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev2\",\"status\":\"error\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"error\"}\n","{\"device\":\"Dev2\",\"status\":\"ok\"}\n","{\"device\":\"Dev2\",\"status\":\"error\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n","\n","mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\n","\n","print(\"Source stream created...\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:49:04.1472997Z","session_start_time":null,"execution_start_time":"2024-03-30T21:49:05.0698037Z","execution_finish_time":"2024-03-30T21:49:05.8024416Z","parent_msg_id":"a3d198f0-de23-4504-aaca-1517e54c651c"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 26, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Source stream created...\n"]}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"}},"id":"7924a3b6-0122-4767-9735-6ffbb340e399"},{"cell_type":"markdown","source":["2. In a new code cell, add and run the following code:\n","\n","This code writes the streaming device data in delta format to a folder named **iotdevicedata**. Because the path for the folder location in the **Tables** folder, a table will automatically be created for it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7f643262-0b43-4414-aed8-0e06e0a69ed3"},{"cell_type":"code","source":["%%pyspark\n","\n","# Write the stream to a delta table\n","delta_stream_table_path = 'Tables/iotdevicedata'\n","checkpointpath          = 'Files/delta/checkpoint'\n","deltastream             = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\n","\n","print(\"Streaming to delta sink...\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":27,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:49:27.1158257Z","session_start_time":null,"execution_start_time":"2024-03-30T21:49:27.4195618Z","execution_finish_time":"2024-03-30T21:49:28.2058685Z","parent_msg_id":"a5c89875-0f57-477f-815b-3279f748159a"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 27, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Streaming to delta sink...\n"]}],"execution_count":22,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"}},"id":"441c2e66-f249-49b0-ad85-e8b4d897b021"},{"cell_type":"markdown","source":["3. In a new code cell, add and run the following code:\n","\n","This code queries the **iotdevicedata** table, which contains the device data from the streaming source."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5073257c-31c4-4983-a4ec-bbeecbe651f2"},{"cell_type":"code","source":["%%sql\n","\n","SELECT * FROM iotdevicedata;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":28,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:49:30.8290996Z","session_start_time":null,"execution_start_time":"2024-03-30T21:49:31.1540664Z","execution_finish_time":"2024-03-30T21:49:34.620524Z","parent_msg_id":"2094438e-a80a-4b9a-8949-7c8771174255"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 28, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":23,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"device","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}}]},"data":[["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev2","error"],["Dev1","ok"],["Dev1","error"],["Dev2","ok"],["Dev2","error"],["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev1","error"],["Dev2","error"],["Dev1","ok"]]},"text/plain":"<Spark SQL result set with 16 rows and 2 fields>"},"metadata":{}}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"722af5a1-04ff-4c6f-9bcb-72a21ec93ba0"},{"cell_type":"markdown","source":["4. In a new code cell, add and run the following code:\n","\n","This code writes more hypothetical device data to the streaming source."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8860a6d0-29f1-46f0-b610-7fb0c4881fb8"},{"cell_type":"code","source":["%%pyspark\n","\n","# Add more data to the source stream\n","more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}\n","{\"device\":\"Dev1\",\"status\":\"error\"}\n","{\"device\":\"Dev2\",\"status\":\"error\"}\n","{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n","\n","mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":29,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:49:38.4173716Z","session_start_time":null,"execution_start_time":"2024-03-30T21:49:38.7447984Z","execution_finish_time":"2024-03-30T21:49:39.4924329Z","parent_msg_id":"f704358b-ff5b-4761-8fe3-705dc23c7c49"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 29, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":24,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"}},"id":"bf8eeb38-98ef-484b-a48f-2e094cca205d"},{"cell_type":"markdown","source":["5. Re-run the cell containing the following code:\n","\n","This code queries the **iotdevicedata** table again, which should now include the additional data that was added to the streaming source."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"453a7ca7-e964-449d-b534-0a503bc9ce86"},{"cell_type":"code","source":["%%sql\n","\n","SELECT * FROM iotdevicedata;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":30,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:49:57.8571601Z","session_start_time":null,"execution_start_time":"2024-03-30T21:49:58.2287814Z","execution_finish_time":"2024-03-30T21:49:59.6859311Z","parent_msg_id":"97fbed76-d686-4031-b29f-d87d75d159e5"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 30, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":25,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"device","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}}]},"data":[["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev2","error"],["Dev1","ok"],["Dev1","error"],["Dev2","ok"],["Dev2","error"],["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev1","ok"],["Dev1","error"],["Dev2","error"],["Dev1","ok"]]},"text/plain":"<Spark SQL result set with 16 rows and 2 fields>"},"metadata":{}}],"execution_count":25,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"f6c59209-04a1-4bf0-9460-ba1790a9dcab"},{"cell_type":"markdown","source":["6. In a new code cell, add and run the following code:\n","\n","This code stops the stream."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f2efbad6-4742-473c-8589-142d2919e0c2"},{"cell_type":"code","source":["%%pyspark\n","\n","deltastream.stop()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":31,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:50:03.009433Z","session_start_time":null,"execution_start_time":"2024-03-30T21:50:03.3338841Z","execution_finish_time":"2024-03-30T21:50:03.5471765Z","parent_msg_id":"65f0761c-c147-47de-a1ad-77bd68c21695"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 31, Finished, Available)"},"metadata":{}}],"execution_count":26,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"}},"id":"e320ebd6-c6a5-4f62-8d32-12caed5df17e"},{"cell_type":"markdown","source":["## Save the notebook and end the Spark session\n","Now that you’ve finished working with the data, you can save the notebook with a meaningful name and end the Spark session.\n","\n","1. In the notebook menu bar, use the **⚙️ Settings** icon to view the notebook settings.\n","2. Set the **Name** of the notebook to **Use Delta Tables in Apache Spark - R version**, and then close the settings pane.\n","3. Run the following code to **disconnect** the Spark connection:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6d6a9909-607b-4bcb-8db2-15a723152898"},{"cell_type":"code","source":["# Disconnect the Spark connection \n","spark_disconnect(sc)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"434f51d4-1857-4492-9ba0-b0be66a4c065","statement_id":32,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-30T21:50:06.3812872Z","session_start_time":null,"execution_start_time":"2024-03-30T21:50:06.7068668Z","execution_finish_time":"2024-03-30T21:50:06.9364598Z","parent_msg_id":"283383c0-49b3-4326-8e42-5fad50a4b17f"},"text/plain":"StatementMeta(, 434f51d4-1857-4492-9ba0-b0be66a4c065, 32, Finished, Available)"},"metadata":{}}],"execution_count":27,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"33960753-cb94-40d9-b0c2-3c7705ef8c49"}],"metadata":{"language_info":{"name":"r"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"r","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"bebf1738-538c-413d-8b17-0ffa6d0adc24","known_lakehouses":[{"id":"bebf1738-538c-413d-8b17-0ffa6d0adc24"}],"default_lakehouse_name":"dp_uusiuusi","default_lakehouse_workspace_id":"3cb84d34-a5d7-41ef-91ee-d568529315ad"}}},"nbformat":4,"nbformat_minor":5}